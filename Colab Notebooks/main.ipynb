{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"EnC-RoMHCYq7"},"outputs":[],"source":["import time\n","import glob\n","import os\n","import cv2\n","import numpy as np\n","import sys\n","import glob\n","import random\n","import importlib.util\n","from tensorflow.lite.python.interpreter import Interpreter\n","\n","import matplotlib\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KiBo3-NpC4X5"},"outputs":[],"source":["from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I2w4Af72CYq9"},"outputs":[],"source":["\n","# Import packages\n","import os\n","import cv2\n","import numpy as np\n","import sys\n","import glob\n","import random\n","import importlib.util\n","from tensorflow.lite.python.interpreter import Interpreter\n","\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline\n","\n","### Define function for inferencing with TFLite model and displaying results\n","\n","def tflite_detect_images(modelpath, imgpath, lblpath, min_conf=0.5, num_test_images=10, savepath='/content/results', txt_only=False):\n","\n","  # Grab filenames of all images in test folder\n","  images = glob.glob(imgpath + '/*.jpg') + glob.glob(imgpath + '/*.JPG') + glob.glob(imgpath + '/*.png') + glob.glob(imgpath + '/*.bmp')\n","\n","  # Load the label map into memory\n","  with open(lblpath, 'r') as f:\n","      labels = [line.strip() for line in f.readlines()]\n","\n","  # Load the Tensorflow Lite model into memory\n","  interpreter = Interpreter(model_path=modelpath)\n","  interpreter.allocate_tensors()\n","\n","  # Get model details\n","  input_details = interpreter.get_input_details()\n","  output_details = interpreter.get_output_details()\n","  height = input_details[0]['shape'][1]\n","  width = input_details[0]['shape'][2]\n","\n","  float_input = (input_details[0]['dtype'] == np.float32)\n","\n","  input_mean = 127.5\n","  input_std = 127.5\n","\n","  # Randomly select test images\n","  images_to_test = random.sample(images, num_test_images)\n","\n","  # Loop over every image and perform detection\n","  for image_path in images_to_test:\n","\n","      # Load image and resize to expected shape [1xHxWx3]\n","      image = cv2.imread(image_path)\n","      image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","      imH, imW, _ = image.shape\n","      image_resized = cv2.resize(image_rgb, (width, height))\n","      input_data = np.expand_dims(image_resized, axis=0)\n","\n","      # Normalize pixel values if using a floating model (i.e. if model is non-quantized)\n","      if float_input:\n","          input_data = (np.float32(input_data) - input_mean) / input_std\n","\n","      # Perform the actual detection by running the model with the image as input\n","      interpreter.set_tensor(input_details[0]['index'],input_data)\n","      interpreter.invoke()\n","\n","      # Retrieve detection results\n","      boxes = interpreter.get_tensor(output_details[1]['index'])[0] # Bounding box coordinates of detected objects\n","      classes = interpreter.get_tensor(output_details[3]['index'])[0] # Class index of detected objects\n","      scores = interpreter.get_tensor(output_details[0]['index'])[0] # Confidence of detected objects\n","\n","      detections = []\n","\n","      # Loop over all detections and draw detection box if confidence is above minimum threshold\n","      for i in range(len(scores)):\n","          if ((scores[i] > min_conf) and (scores[i] <= 1.0)):\n","\n","                # Get bounding box coordinates and draw box\n","                # Interpreter can return coordinates that are outside of image dimensions, need to force them to be within image using max() and min()\n","                ymin = int(max(1,(boxes[i][0] * imH)))\n","                xmin = int(max(1,(boxes[i][1] * imW)))\n","                ymax = int(min(imH,(boxes[i][2] * imH)))\n","                xmax = int(min(imW,(boxes[i][3] * imW)))\n","\n","                cv2.rectangle(image, (xmin,ymin), (xmax,ymax), (10, 255, 0), 2)\n","\n","                # Draw label\n","                object_name = labels[int(classes[i])] # Look up object name from \"labels\" array using class index\n","                label = '%s: %d%%' % (object_name, int(scores[i]*100)) # Example: 'person: 72%'\n","                labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.4, 1) # Get font size with scale 0.4\n","                label_ymin = max(ymin, labelSize[1] + 10) # Make sure not to draw label too close to top of window\n","                cv2.rectangle(image, (xmin, label_ymin-labelSize[1]-10), (xmin+labelSize[0], label_ymin+baseLine-10), (255, 255, 255), cv2.FILLED) # Draw white box to put label text in\n","                cv2.putText(image, label, (xmin, label_ymin-7), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 0, 0), 1) # Draw label text with scale 0.4\n","\n","                detections.append([object_name, scores[i], xmin, ymin, xmax, ymax])\n","\n","\n","      # All the results have been drawn on the image, now display the image\n","      if txt_only == False: # \"text_only\" controls whether we want to display the image results or just save them in .txt files\n","        image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n","        plt.figure(figsize=(6, 6))  # Adjust the figure size as needed\n","        plt.imshow(image)\n","        plt.axis('off')  # Hide axis\n","        plt.show()\n","\n","      # Save detection results in .txt files (for calculating mAP)\n","      elif txt_only == True:\n","\n","        # Get filenames and paths\n","        image_fn = os.path.basename(image_path)\n","        base_fn, ext = os.path.splitext(image_fn)\n","        txt_result_fn = base_fn +'.txt'\n","        txt_savepath = os.path.join(savepath, txt_result_fn)\n","\n","        # Write results to text file\n","\n","        with open(txt_savepath,'w') as f:\n","            for detection in detections:\n","                f.write('%s %.4f %d %d %d %d\\n' % (detection[0], detection[1], detection[2], detection[3], detection[4], detection[5]))\n","\n","  return\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":499,"output_embedded_package_id":"1HyOQfT3Q-OWhXtCNLyONZhCD0157QcwI"},"id":"vJ148ZhgCYq-","outputId":"ad794471-3759-4050-b3c3-92da8ebc1127"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# Define paths\n","model_path = \"/content/drive/MyDrive/detect.tflite\"\n","images_dir = \"/content/drive/MyDrive/test_data\"\n","label_path = \"/content/drive/MyDrive/labels.txt\"\n","\n","# Perform inference and display results\n","tflite_detect_images(model_path, images_dir, label_path, min_conf=0.55, num_test_images=10, txt_only=False)\n"]},{"cell_type":"markdown","metadata":{"id":"yE-PS9BBCYq-"},"source":["To use with your cam"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B3lDRySSCYq_","outputId":"b5bf03a7-b4aa-4d97-c4b1-ceebda6635b3"},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-04-14 23:31:05.596014: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2024-04-14 23:31:05.632928: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-04-14 23:31:06.428984: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n","qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"/home/hamza/.local/lib/python3.10/site-packages/cv2/qt/plugins\"\n"]}],"source":["# Import packages\n","import cv2\n","import numpy as np\n","import sys\n","import importlib.util\n","from tensorflow.lite.python.interpreter import Interpreter\n","\n","### Define function for inferencing with TFLite model and displaying results\n","\n","def tflite_detect_webcam(modelpath, lblpath, min_conf=0.5):\n","\n","    # Load the label map into memory\n","    with open(lblpath, 'r') as f:\n","        labels = [line.strip() for line in f.readlines()]\n","\n","    # Load the TensorFlow Lite model into memory\n","    interpreter = Interpreter(model_path=modelpath)\n","    interpreter.allocate_tensors()\n","\n","    # Get model details\n","    input_details = interpreter.get_input_details()\n","    output_details = interpreter.get_output_details()\n","    height = input_details[0]['shape'][1]\n","    width = input_details[0]['shape'][2]\n","\n","    float_input = (input_details[0]['dtype'] == np.float32)\n","\n","    input_mean = 127.5\n","    input_std = 127.5\n","\n","    # Open webcam\n","    cap = cv2.VideoCapture(0)\n","\n","    while True:\n","        # Read frame from webcam\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","\n","        # Resize frame to expected shape [1xHxWx3]\n","        frame_resized = cv2.resize(frame, (width, height))\n","\n","        # Normalize pixel values if using a floating model (i.e., if the model is non-quantized)\n","        if float_input:\n","            input_data = (np.float32(frame_resized) - input_mean) / input_std\n","        else:\n","            input_data = np.uint8(frame_resized)\n","\n","        # Perform the actual detection by running the model with the frame as input\n","        interpreter.set_tensor(input_details[0]['index'], np.expand_dims(input_data, axis=0))\n","        interpreter.invoke()\n","\n","        # Retrieve detection results\n","        boxes = interpreter.get_tensor(output_details[1]['index'])[0]  # Bounding box coordinates of detected objects\n","        classes = interpreter.get_tensor(output_details[3]['index'])[0]  # Class index of detected objects\n","        scores = interpreter.get_tensor(output_details[0]['index'])[0]  # Confidence of detected objects\n","\n","        # Loop over all detections and draw detection box if confidence is above minimum threshold\n","        for i in range(len(scores)):\n","            if ((scores[i] > min_conf) and (scores[i] <= 1.0)):\n","                # Get bounding box coordinates and draw box\n","                ymin = int(max(1, (boxes[i][0] * frame.shape[0])))\n","                xmin = int(max(1, (boxes[i][1] * frame.shape[1])))\n","                ymax = int(min(frame.shape[0], (boxes[i][2] * frame.shape[0])))\n","                xmax = int(min(frame.shape[1], (boxes[i][3] * frame.shape[1])))\n","\n","                cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (10, 255, 0), 2)\n","\n","                # Draw label\n","                object_name = labels[int(classes[i])]  # Look up object name from \"labels\" array using class index\n","                label = '%s: %d%%' % (object_name, int(scores[i] * 100))  # Example: 'person: 72%'\n","                cv2.putText(frame, label, (xmin, ymin - 7), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 0, 255), 1)\n","\n","        # Display frame\n","        cv2.imshow('Webcam', frame)\n","\n","        # Exit if 'q' is pressed\n","        if cv2.waitKey(1) & 0xFF == ord('q'):\n","            break\n","\n","    # Release webcam and close windows\n","    cap.release()\n","    cv2.destroyAllWindows()\n","\n","# Call the function for webcam inference\n","tflite_detect_webcam(modelpath='/home/hamza/mobilenetv2-ssd/detect.tflite', lblpath='/home/hamza/mobilenetv2-ssd/labels.txt')\n"]},{"cell_type":"markdown","metadata":{"id":"xWb9Lz0NCYrA"},"source":["# FPS"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"6BVbz2xCCYrA"},"outputs":[],"source":["import time\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","def tflite_detect_images_fps(modelpath, imgpath, lblpath, min_conf=0.5, num_test_images=700, savepath='/content/results', txt_only=False):\n","\n","    # Grab filenames of all images in test folder\n","    images = glob.glob(imgpath + '/*.jpg') + glob.glob(imgpath + '/*.JPG') + glob.glob(imgpath + '/*.png') + glob.glob(imgpath + '/*.bmp')\n","\n","    # Load the label map into memory\n","    with open(lblpath, 'r') as f:\n","        labels = [line.strip() for line in f.readlines()]\n","\n","    # Load the Tensorflow Lite model into memory\n","    interpreter = Interpreter(model_path=modelpath)\n","    interpreter.allocate_tensors()\n","\n","    # Get model details\n","    input_details = interpreter.get_input_details()\n","    output_details = interpreter.get_output_details()\n","    height = input_details[0]['shape'][1]\n","    width = input_details[0]['shape'][2]\n","\n","    float_input = (input_details[0]['dtype'] == np.float32)\n","\n","    input_mean = 127.5\n","    input_std = 127.5\n","\n","    # Lists to store inference times and FPS values\n","    inference_times = []\n","    fps_values = []\n","\n","    # Randomly select test images\n","    images_to_test = random.sample(images, num_test_images)\n","\n","    # Loop over every image and perform detection\n","    for image_path in images_to_test:\n","\n","        # Load image and resize to expected shape [1xHxWx3]\n","        image = cv2.imread(image_path)\n","        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        imH, imW, _ = image.shape\n","        image_resized = cv2.resize(image_rgb, (width, height))\n","        input_data = np.expand_dims(image_resized, axis=0)\n","\n","        # Normalize pixel values if using a floating model (i.e. if model is non-quantized)\n","        if float_input:\n","            input_data = (np.float32(input_data) - input_mean) / input_std\n","\n","        # Perform the actual detection by running the model with the image as input\n","        start_time = time.time()\n","        interpreter.set_tensor(input_details[0]['index'],input_data)\n","        interpreter.invoke()\n","        end_time = time.time()\n","\n","        # Calculate inference time\n","        inference_time = end_time - start_time\n","        inference_times.append(inference_time)\n","\n","        # Calculate FPS\n","        fps = 1 / inference_time\n","        fps_values.append(fps)\n","\n","        # Retrieve detection results\n","        boxes = interpreter.get_tensor(output_details[1]['index'])[0] # Bounding box coordinates of detected objects\n","        classes = interpreter.get_tensor(output_details[3]['index'])[0] # Class index of detected objects\n","        scores = interpreter.get_tensor(output_details[0]['index'])[0] # Confidence of detected objects\n","\n","        detections = []\n","\n","        # Loop over all detections and draw detection box if confidence is above minimum threshold\n","        for i in range(len(scores)):\n","            if ((scores[i] > min_conf) and (scores[i] <= 1.0)):\n","\n","                # Get bounding box coordinates and draw box\n","                # Interpreter can return coordinates that are outside of image dimensions, need to force them to be within image using max() and min()\n","                ymin = int(max(1,(boxes[i][0] * imH)))\n","                xmin = int(max(1,(boxes[i][1] * imW)))\n","                ymax = int(min(imH,(boxes[i][2] * imH)))\n","                xmax = int(min(imW,(boxes[i][3] * imW)))\n","\n","                detections.append([labels[int(classes[i])], scores[i], xmin, ymin, xmax, ymax])\n","\n","        # Print average inference time and FPS on the image\n","        #print(\"Average inference time on\", os.path.basename(image_path), \":\", inference_time)\n","        #print(\"FPS on\", os.path.basename(image_path), \":\", fps)\n","\n","    # Calculate average FPS\n","    average_fps = np.mean(fps_values)\n","    print(\"Average FPS:\", average_fps)\n","\n","    # Plot graphs\n","    plt.figure(figsize=(10, 5))\n","\n","    # Plot FPS\n","    plt.subplot(1, 2, 1)\n","    plt.plot(fps_values, label='FPS')\n","    plt.axhline(y=average_fps, color='r', linestyle='--', label='Average FPS: {:.2f}'.format(average_fps))\n","    plt.xlabel('Image Index')\n","    plt.ylabel('FPS')\n","    plt.title('FPS')\n","    plt.legend()\n","\n","    # Plot average inference time\n","    plt.subplot(1, 2, 2)\n","    plt.plot(inference_times, label='Average Inference Time')\n","    plt.xlabel('Image Index')\n","    plt.ylabel('Average Inference Time (s)')\n","    plt.title('Average Inference Time')\n","    plt.legend()\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","    return\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wOdPpqHcCYrA"},"outputs":[],"source":["\n","modelpath = \"/content/detect.tflite\"\n","imgpath = \"/content/drive/MyDrive/test_data\"\n","lblpath = \"/content/labels.txt\"\n","tflite_detect_images_fps(modelpath, imgpath, lblpath)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"J0CjR9wcCYrA"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}